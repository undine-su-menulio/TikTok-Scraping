{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å!\n",
    "\n",
    "–≠—Ç–æ —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö, —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –ø–æ–º–æ—â–∏ —Ñ–∞–π–ª–∞ TikTok Scraping.\n",
    "\n",
    "–î–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã –∑–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫–∏ –≤ —Ä–∞–∑–¥–µ–ª–µ \"–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ\", –∞ –∑–∞—Ç–µ–º –ø–µ—Ä–µ–π–¥–∏—Ç–µ –∫ —è—á–µ–π–∫–∞–º –Ω—É–∂–Ω–æ–≥–æ –±–ª–æ–∫–∞.\n",
    "\n",
    "–ß—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å —Ñ–∞–π–ª, –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ –º—ã –±—É–¥–µ–º –≤—ã–≤–æ–¥–∏—Ç—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–∞ –ø–æ –ø—è—Ç—å —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–õ–æ–≥–∏–∫–∞ —Ä–∞–±–æ—Ç—ã –∫–æ–¥–∞: \n",
    "\n",
    "1. –°–æ–∑–¥–∞–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –≤—Å–µ—Ö unique_characteristics\n",
    "2. –î–ª—è –∫–∞–∂–¥–æ–π –ø–æ–¥–ø–∞–ø–∫–∏ –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏\n",
    "3. –í–æ–∑—å–º–∏ —Ñ–∞–π–ª unique_characteristics\n",
    "4. –î–æ–±–∞–≤—å –¥–∞–Ω–Ω—ã–µ –∫ –æ–±—â–µ–º—É –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—É\n",
    "5. –°–æ—Ö—Ä–∞–Ω–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –∫–∞–∫ —Ñ–∞–π–ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'americans',\n",
       " 'balkan',\n",
       " 'baltic',\n",
       " 'chamoney1',\n",
       " 'easterneuropean',\n",
       " 'easterneuropeancheck',\n",
       " 'helvetica12',\n",
       " 'katteryyna',\n",
       " 'katteryyna_stitch',\n",
       " 'miadio',\n",
       " 'monica_zielinski',\n",
       " 'nikiproshin',\n",
       " 'okaykali',\n",
       " 'postsoviet',\n",
       " 'SepVideos',\n",
       " 'slavic',\n",
       " 'straightouttarussia',\n",
       " 'teameffujoe',\n",
       " 'TellMeYouNotAmerican',\n",
       " 'therussianmatreshka',\n",
       " 'torryhermann',\n",
       " 'ugneexo',\n",
       " 'voidable',\n",
       " 'voidable2',\n",
       " 'webkinpoodel']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω—É–∂–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pymorphy2 as pm\n",
    "\n",
    "def lemmatize_nltk(fname, text):\n",
    "    \"\"\"\n",
    "    fname: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞, –∫—É–¥–∞ –±—É–¥–µ—Ç –∑–∞–ø–∏—Å–∞–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏\n",
    "    text: —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    wl = WordNetLemmatizer()\n",
    "    \n",
    "    with open (fname, 'w', encoding='utf-8') as outfile:\n",
    "        for i in range(len(tagged)):\n",
    "            if tagged[i][1].startswith('N'):\n",
    "                outfile.write(wl.lemmatize(tagged[i][0], pos='n') + ' ')\n",
    "            elif tagged[i][1].startswith('V') or tagged[i][1].startswith('M'):\n",
    "                outfile.write(wl.lemmatize(tagged[i][0], pos='v') + ' ')\n",
    "            else:\n",
    "                outfile.write(tagged[i][0] + ' ')\n",
    "\n",
    "# –í—ã–≤–µ–¥–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø–∞–ø–æ–∫ –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –º—ã –±—É–¥–µ–º –ø–æ–¥—Ç—è–≥–∏–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ \n",
    "                \n",
    "folders_list_all = os.scandir (os.getcwd())\n",
    "folders_list_dir = []\n",
    "for i in folders_list_all:\n",
    "    if i.is_dir() == True:\n",
    "        folders_list_dir.append (i.name)\n",
    "folders_list_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–¥–ø–∏—Å–µ–π, —Å—Ç–∏–∫–µ—Ä–æ–≤ –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–π —Ä–µ—á–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['americans_unique_characteristics.csv',\n",
       " 'balkan_unique_characteristics.csv',\n",
       " 'baltic_unique_characteristics.csv',\n",
       " 'chamoney1_unique_characteristics.csv',\n",
       " 'easterneuropean_unique_characteristics.csv',\n",
       " 'easterneuropeancheck_unique_characteristics.csv',\n",
       " 'helvetica12_unique_characteristics.csv',\n",
       " 'katteryyna_unique_characteristics.csv',\n",
       " 'katteryyna_unique_characteristics.csv',\n",
       " 'miadio_unique_characteristics.csv',\n",
       " 'monica_zielinski_unique_characteristics.csv',\n",
       " 'nikiproshin_unique_characteristics.csv',\n",
       " 'okaykali_unique_characteristics.csv',\n",
       " 'postsoviet_unique_characteristics.csv',\n",
       " 'SepVideos_unique_characteristics.csv',\n",
       " 'slavic_unique_characteristics.csv',\n",
       " 'straightouttarussia_unique_characteristics.csv',\n",
       " 'teameffujoe_unique_characteristics.csv',\n",
       " 'TellMeYouNotAmerican_unique_characteristics.csv',\n",
       " 'therussianmatreshka_unique_characteristics.csv',\n",
       " 'torryhermann_unique_characteristics.csv',\n",
       " 'ugneexo_unique_characteristics.csv',\n",
       " 'voidable_unique_characteristics.csv',\n",
       " 'voidable2_unique_characteristics.csv',\n",
       " 'webkinpoodel_unique_characteristics.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤ –æ–¥–Ω—É —Ç–∞–±–ª–∏—Ü—É\n",
    "\n",
    "path = os.getcwd()\n",
    "folders_subfiles = []\n",
    "for i in folders_list_dir:\n",
    "    folders_subfiles.append (os.listdir (os.path.join (path, i)))\n",
    "all_csv = []\n",
    "for i in folders_subfiles:\n",
    "    for j in i:\n",
    "        if j[-26:] == 'unique_characteristics.csv':\n",
    "            all_csv.append (j)\n",
    "all_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>author_nickname</th>\n",
       "      <th>aweme_id</th>\n",
       "      <th>comments</th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>likes</th>\n",
       "      <th>link</th>\n",
       "      <th>music</th>\n",
       "      <th>shares</th>\n",
       "      <th>sticker_text</th>\n",
       "      <th>stitched_original_link</th>\n",
       "      <th>stt_text</th>\n",
       "      <th>timestamp_create_time</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6866876516039623686</td>\n",
       "      <td>dMs dOnT wORk</td>\n",
       "      <td>._viv</td>\n",
       "      <td>6913612689344400642</td>\n",
       "      <td>7918</td>\n",
       "      <td>03.01.2021</td>\n",
       "      <td>#fyp #foryou #lmao #animals #russia #usa #amer...</td>\n",
       "      <td>['#fyp', '#foryou', '#lmao', '#animals', '#rus...</td>\n",
       "      <td>1200000</td>\n",
       "      <td>https://www.tiktok.com/@._viv/video/6913612689...</td>\n",
       "      <td>Originalton</td>\n",
       "      <td>36300</td>\n",
       "      <td>Americans üá∫üá∏: we have the best trained animal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no no no no i don't either right</td>\n",
       "      <td>1609700895</td>\n",
       "      <td>6900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>313224616116707328</td>\n",
       "      <td>Louis Oro</td>\n",
       "      <td>louisoro</td>\n",
       "      <td>6906426527299161349</td>\n",
       "      <td>6747</td>\n",
       "      <td>15.12.2020</td>\n",
       "      <td>#stitch with @voidable_  inspired by @maaaritz...</td>\n",
       "      <td>['#tiktokparati', '#bellezalatina', '#carcajad...</td>\n",
       "      <td>738400</td>\n",
       "      <td>https://www.tiktok.com/@louisoro/video/6906426...</td>\n",
       "      <td>original sound</td>\n",
       "      <td>3721</td>\n",
       "      <td>CC:QU√â ES LO M√ÅS TONTO QUE UN ESTADOUNIDENSE ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i'm calling the curious what is the dumbest t...</td>\n",
       "      <td>1608027941</td>\n",
       "      <td>3100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6800387735624893445</td>\n",
       "      <td>Cody Wayne</td>\n",
       "      <td>codywaynestout</td>\n",
       "      <td>6901458722493664518</td>\n",
       "      <td>8675</td>\n",
       "      <td>02.12.2020</td>\n",
       "      <td>ü§¶üèº‚Äç‚ôÇÔ∏è #fyp #fyp„Ç∑ #fight #round2 #americans #cd...</td>\n",
       "      <td>['#fyp', '#fyp„Ç∑', '#fight', '#round2', '#ameri...</td>\n",
       "      <td>675000</td>\n",
       "      <td>https://www.tiktok.com/@codywaynestout/video/6...</td>\n",
       "      <td>original sound</td>\n",
       "      <td>94300</td>\n",
       "      <td>The CDC Americans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>we have approved a vaccine for nine it's appr...</td>\n",
       "      <td>1606871108</td>\n",
       "      <td>3500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6532295762217320449</td>\n",
       "      <td>usherüçú</td>\n",
       "      <td>theasianusher</td>\n",
       "      <td>6949923736741088517</td>\n",
       "      <td>10900</td>\n",
       "      <td>11.04.2021</td>\n",
       "      <td>American humour at its finestüòÇ #fyp #foryou #v...</td>\n",
       "      <td>['#fyp', '#foryou', '#viral', '#trending', '#b...</td>\n",
       "      <td>643700</td>\n",
       "      <td>https://www.tiktok.com/@theasianusher/video/69...</td>\n",
       "      <td>original sound</td>\n",
       "      <td>4087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the deodorant as something that england has y...</td>\n",
       "      <td>1618155220</td>\n",
       "      <td>1900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>141043686905278464</td>\n",
       "      <td>Michael Vorpahl</td>\n",
       "      <td>michaelvorpahl</td>\n",
       "      <td>6914793867334323462</td>\n",
       "      <td>16600</td>\n",
       "      <td>06.01.2021</td>\n",
       "      <td>This absolutely blows my mind... #twitter #soc...</td>\n",
       "      <td>['#twitter', '#social', '#president', '#speech...</td>\n",
       "      <td>558900</td>\n",
       "      <td>https://www.tiktok.com/@michaelvorpahl/video/6...</td>\n",
       "      <td>original sound</td>\n",
       "      <td>46800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no is check this out to our beloved still rem...</td>\n",
       "      <td>1609975921</td>\n",
       "      <td>2800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author_id      author_name author_nickname             aweme_id  \\\n",
       "0  6866876516039623686    dMs dOnT wORk           ._viv  6913612689344400642   \n",
       "1   313224616116707328        Louis Oro        louisoro  6906426527299161349   \n",
       "2  6800387735624893445       Cody Wayne  codywaynestout  6901458722493664518   \n",
       "3  6532295762217320449           usherüçú   theasianusher  6949923736741088517   \n",
       "4   141043686905278464  Michael Vorpahl  michaelvorpahl  6914793867334323462   \n",
       "\n",
       "   comments        date                                        description  \\\n",
       "0      7918  03.01.2021  #fyp #foryou #lmao #animals #russia #usa #amer...   \n",
       "1      6747  15.12.2020  #stitch with @voidable_  inspired by @maaaritz...   \n",
       "2      8675  02.12.2020  ü§¶üèº‚Äç‚ôÇÔ∏è #fyp #fyp„Ç∑ #fight #round2 #americans #cd...   \n",
       "3     10900  11.04.2021  American humour at its finestüòÇ #fyp #foryou #v...   \n",
       "4     16600  06.01.2021  This absolutely blows my mind... #twitter #soc...   \n",
       "\n",
       "                                            hashtags    likes  \\\n",
       "0  ['#fyp', '#foryou', '#lmao', '#animals', '#rus...  1200000   \n",
       "1  ['#tiktokparati', '#bellezalatina', '#carcajad...   738400   \n",
       "2  ['#fyp', '#fyp„Ç∑', '#fight', '#round2', '#ameri...   675000   \n",
       "3  ['#fyp', '#foryou', '#viral', '#trending', '#b...   643700   \n",
       "4  ['#twitter', '#social', '#president', '#speech...   558900   \n",
       "\n",
       "                                                link           music  shares  \\\n",
       "0  https://www.tiktok.com/@._viv/video/6913612689...     Originalton   36300   \n",
       "1  https://www.tiktok.com/@louisoro/video/6906426...  original sound    3721   \n",
       "2  https://www.tiktok.com/@codywaynestout/video/6...  original sound   94300   \n",
       "3  https://www.tiktok.com/@theasianusher/video/69...  original sound    4087   \n",
       "4  https://www.tiktok.com/@michaelvorpahl/video/6...  original sound   46800   \n",
       "\n",
       "                                        sticker_text stitched_original_link  \\\n",
       "0   Americans üá∫üá∏: we have the best trained animal...                    NaN   \n",
       "1   CC:QU√â ES LO M√ÅS TONTO QUE UN ESTADOUNIDENSE ...                    NaN   \n",
       "2                                  The CDC Americans                    NaN   \n",
       "3                                                NaN                    NaN   \n",
       "4                                                NaN                    NaN   \n",
       "\n",
       "                                            stt_text  timestamp_create_time  \\\n",
       "0                   no no no no i don't either right             1609700895   \n",
       "1   i'm calling the curious what is the dumbest t...             1608027941   \n",
       "2   we have approved a vaccine for nine it's appr...             1606871108   \n",
       "3   the deodorant as something that england has y...             1618155220   \n",
       "4   no is check this out to our beloved still rem...             1609975921   \n",
       "\n",
       "     views  \n",
       "0  6900000  \n",
       "1  3100000  \n",
       "2  3500000  \n",
       "3  1900000  \n",
       "4  2800000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –û–±—ä–µ–¥–∏–Ω–∏–º –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã –≤ –µ–¥–∏–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º\n",
    "\n",
    "def get_folders_name (filename):\n",
    "    x = filename.split ('_unique_characteristics.csv')\n",
    "    return x[0]\n",
    "\n",
    "all_csv_df = pd.DataFrame ()\n",
    "for i in all_csv:\n",
    "    new_df = pd.read_csv (os.path.join (path, get_folders_name (i), all_csv [all_csv.index(i)])).drop ('Unnamed: 0', axis = 1)\n",
    "    all_csv_df = all_csv_df.append (new_df)\n",
    "    all_csv_df.reset_index().drop_duplicates()\n",
    "all_csv_df = all_csv_df.drop_duplicates()\n",
    "all_csv_df [:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω–∏–º –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –≤ csv-—Ñ–∞–π–ª\n",
    "\n",
    "pd.DataFrame.to_csv (all_csv_df, 'all_unique_characteristics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü–æ–∏—Å–∫ –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in all_csv_df.iterrows():\n",
    "    try:\n",
    "        if 'switzerland' in i[1]['hashtags']:\n",
    "            print (i[1]['author_nickname'])\n",
    "            print (i[1]['description'])\n",
    "            print (i[1]['sticker_text'])\n",
    "            print (i[1]['stt_text'])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "retroriga = []\n",
    "for i in all_csv_df.iterrows ():\n",
    "    try:\n",
    "        if ('easterneuropeancheck' in i[1]['hashtags']) & ('washingtoncheck' in i[1]['hashtags']):\n",
    "            print (i[1]['description'])\n",
    "            print (i[1]['sticker_text'])\n",
    "            print (i[1]['author_nickname'])\n",
    "            print (i[1]['aweme_id'])\n",
    "            print (i[1]['stt_text'])\n",
    "#             retroriga.append (i[1]['author_nickname'])\n",
    "    except TypeError:\n",
    "        pass\n",
    "# len (retroriga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ –ø–æ–¥–ø–∏—Å–µ–π –∏ —Å—Ç–∏–∫–µ—Ä–æ–≤\n",
    "\n",
    "–ó–∞–ø—É—Å–∫–∞—Ç—å –ø–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –±–ª–æ–∫–∞ \"–î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–¥–ø–∏—Å–µ–π, —Å—Ç–∏–∫–µ—Ä–æ–≤ –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–π —Ä–µ—á–∏\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#fyp #foryou #lmao #animals #russia #usa #america #americans #tiktok #dog #bird #putin #pigeon  Americans üá∫üá∏: we have the best trained animals  Russia üá∑üá∫:',\n",
       " '#stitch with @voidable_  inspired by @maaaritza #tiktokparati #bellezalatina #carcajadas #aquiaprendo #americans #wearefamily #animo #recordsdays #fyp  CC:QU√â ES LO M√ÅS TONTO QUE UN ESTADOUNIDENSE TE HA DICHO? CC:Soy del norte de M√©xico, y hace 2 a√±os estaba en la playa  LOUIS.ORO CC:Yo estaba hablando con un estadunidense, y √©l pens√≥ que yo era brit√°nico  CC:CREO QUE FUE POR MI ACENTO CC:As√≠ que le pregunt√© qu√© pensaba sobre los mexicanos CC: Y el dijo algo como esto CC: Los mexicanos no prosperan porque son morenos, chaparros, feos CC:Comen frijoles todos los d√≠as y no tienen educaci√≥n. CC:As√≠ que se tienen que ir a mi pa√≠s  CC:Y luego yo dije CC:¬øEst√°s demente? CC:Soy mexicano, bro CC:No hay nada malo con ser moreno CC: Y s√≠,  me gustan los frijoles  CC:Pero hoy cen√© langosta  CC:Y soy como 10 cm m√°s alto que t√∫ CC:Y por cierto... CC:Creo que soy m√°s guapo que t√∫',\n",
       " 'ü§¶üèº\\u200d‚ôÇÔ∏è #fyp #fyp„Ç∑ #fight #round2 #americans #cdc #coronavirus #westvirginia #wv #lgbt #unitedstates  The CDC Americans',\n",
       " '@evan.hart  #Americans really don‚Äôt understand most historic facts #usa #america #history #foryou #military #worldwar #worldwarII #WW2 #murica  History do be fun',\n",
       " 'Reply to @napabilirim2 #americaslander #america #americans #asian #asia #asiancountries #joke #map #countries #lol #fyp #foryou  how americans see asia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–æ–¥–ø–∏—Å–∏ –∏ —Å—Ç–∏–∫–µ—Ä—ã –≤ –µ–¥–∏–Ω—ã–π —Å–ø–∏—Å–æ–∫\n",
    "\n",
    "all_desc_n_stickers = []\n",
    "for i in all_csv_df [['description', 'sticker_text']].iterrows():\n",
    "    try:\n",
    "        one_string = i[1]['description'] + ' ' + i[1]['sticker_text']\n",
    "        all_desc_n_stickers.append (one_string)\n",
    "    except TypeError:\n",
    "        pass\n",
    "all_desc_n_stickers [:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ csv-—Ñ–∞–π–ª–∞ —Å –ø–æ–¥–ø–∏—Å—è–º–∏ –∏ —Å—Ç–∏–∫–µ—Ä–∞–º–∏\n",
    "\n",
    "with open ('all_descriptions_and_stickers.csv', 'w', newline = '', encoding = 'utf-8') as all_desc_n_stickers_file:\n",
    "    writer = csv.writer (all_desc_n_stickers_file, delimiter = \",\")\n",
    "    for i in all_desc_n_stickers:\n",
    "        print (i)\n",
    "        writer.writerow ([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ txt-—Ñ–∞–π–ª–∞ —Å –ø–æ–¥–ø–∏—Å—è–º–∏ –∏ —Å—Ç–∏–∫–µ—Ä–∞–º–∏\n",
    "\n",
    "csv_file = 'all_descriptions_and_stickers.csv'\n",
    "txt_file = 'all_descriptions_and_stickers.txt'\n",
    "with open(txt_file, \"w\", encoding = 'utf-8') as my_output_file:\n",
    "    with open(csv_file, \"r\", encoding = 'utf-8') as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏\n",
    "\n",
    "filename = 'all_descriptions_and_stickers.csv'\n",
    "\n",
    "f = open (filename,'r',encoding='utf-8')\n",
    "text = str (f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª\n",
    "\n",
    "lemmatize_nltk ('TRY_lemma_list.txt', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–Ω–æ–≤—å –æ—Ç–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª –∫–∞–∫ —Å–ø–∏—Å–æ–∫ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –ª–µ–º–º\n",
    "\n",
    "with open ('TRY_lemma_list.txt', 'r', encoding='utf-8') as f:\n",
    "    lemmatext = str (f.read()).split ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—á–∏—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –ª–µ–º–º –æ—Ç —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "\n",
    "stop_words = get_stop_words ('english')\n",
    "stop_words.extend (['don', 't', 'gon', 'na', 'm', 's', 'part', 'Part', 'I', 'im', 'You', 'you', 'YOU', 'u', 'i', 'It', 'it',\n",
    "                    'cc', 'CC', 'Cc', 'v', 'stitch', \"n't\", 'go', 'can', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "                    'if', 'If', 'r', 'and', 'And', 'like', 'me', 'Me', 'of', 'Of', 'oh', 'hey', 'a', 'how', 'no', 'are',\n",
    "                   'in', 'the', 'did', 'do', 'didn', 'ya', 're', 'he', 'or', 'to', 'be'])\n",
    "\n",
    "clean = []\n",
    "\n",
    "for lemma in lemmatext:\n",
    "    if lemma not in stop_words and not re.match('\\W+', lemma):\n",
    "        clean.append(lemma.lower())\n",
    "        \n",
    "lemmatext = clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foryou_fyp', 'foryou_lmao', 'animal_lmao', 'animal_russia', 'russia_usa']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –°–æ–µ–¥–∏–Ω—è–µ–º –ª–µ–º–º—ã –ø–æ–ø–∞—Ä–Ω–æ\n",
    "\n",
    "pairs = []\n",
    "\n",
    "for i in range (len(lemmatext)-1):\n",
    "    if lemmatext[i] != lemmatext[i+1]:\n",
    "        pair = min (lemmatext[i], lemmatext[i+1]) + '_' + max (lemmatext[i], lemmatext[i+1])\n",
    "        pairs.append(pair)\n",
    "pairs [:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('foryou_fyp', 408),\n",
       " ('eastern_european', 358),\n",
       " ('foryoupage_fyp', 278),\n",
       " ('foryou_foryoupage', 238),\n",
       " ('balkan_slavic', 136)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å –ø–∞—Ä\n",
    "\n",
    "counter_set_pairs = Counter(pairs).most_common()\n",
    "counter_set_pairs [:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–∫ csv-—Ñ–∞–π–ª –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ –≥—Ä–∞—Ñ\n",
    "\n",
    "csv = 'word1,word2,weight' + '\\n'\n",
    "\n",
    "for count in counter_set_pairs:\n",
    "    if count[1] > 5:\n",
    "        word1 = count[0].split('_')[0]\n",
    "        word2 = count[0].split('_')[1]\n",
    "        weight = str(count[1])\n",
    "        csv += word1 + ',' + word2 + ',' + weight + '\\n'\n",
    "\n",
    "with open('StickersDescription_ethno_counter_pairs.csv', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–π —Ä–µ—á–∏\n",
    "\n",
    "–ó–∞–ø—É—Å–∫–∞—Ç—å –ø–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –±–ª–æ–∫–∞ \"–î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–¥–ø–∏—Å–µ–π, —Å—Ç–∏–∫–µ—Ä–æ–≤ –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–π —Ä–µ—á–∏\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" no no no no i don't either right.\",\n",
       " \" i'm calling the curious what is the dumbest thing in american is if i said to you mm well i am from nothing mexico and she's ago i was in the pitch i was talking to an american in he thought i was british i think it does because of my accent so i asked him what he thinks about mexicans and he said something like this way well mexicans done priest or because they are dark short ugly it being everyday and they have medication so they have to go to my country and then i set are you meant so i am mexican bro this nothing wrong with being dark and yet i like beans that today they had lots tested in and i am about four inches tell the dingy and by the way i think i a handsome than you .\",\n",
       " \" we have approved a vaccine for nine it's approved by scientists if you're just trying to turn isn't just zombies i say why it's my choice will just make it mandatory  yeah.\",\n",
       " \" the deodorant as something that england has yet to discover i bugs bunny way he got an idea from all right mike that you can't be british and asian you need to pick a struggle oh i'm done .\",\n",
       " \" no is check this out to our beloved still remaining commander in chief as of right now this is a video i'm going to play a clip on it of him saying hey i want peace time to go home all of that we have to have peace so go home we love you you're very special you've seen what happens you see the way others traded that are so bad and so evil i know how you feel but go home and go home and peace now check this out because he's actually doing what i believe is the right thing what a lot of people are calling for him to do let's go like it oh wait why can't i like this we try to prevent the tweet like this that otherwise breaks the twitter rules from reaching more people so we disable most of the ways doing gates what the fuck there is no winning but yeah you think there isn't a problem with other things in life with.\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –°–æ–±–∏—Ä–∞–µ–º –≤—Å—é —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—É—é —Ä–µ—á—å –≤ –µ–¥–∏–Ω—ã–π —Å–ø–∏—Å–æ–∫\n",
    "\n",
    "all_stt = []\n",
    "for i in all_csv_df ['stt_text']:\n",
    "    try:\n",
    "        one_string = i + '.'\n",
    "        all_stt.append (one_string)\n",
    "    except TypeError:\n",
    "        pass\n",
    "all_stt [:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–∏—Å–∫ –ø–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–π —Ä–µ—á–∏\n",
    "\n",
    "for i in all_stt:\n",
    "    if ('american' in i) & ('recession' in i):\n",
    "        print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ csv-—Ñ–∞–π–ª–∞ —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–π —Ä–µ—á—å—é\n",
    "\n",
    "with open ('all_stt.csv', 'w', newline = '', encoding = 'utf-8') as all_stt_file:\n",
    "    writer = csv.writer (all_stt_file, delimiter = \",\")\n",
    "    for i in all_stt:        \n",
    "        writer.writerow ([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ txt-—Ñ–∞–π–ª–∞ —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–π —Ä–µ—á—å—é\n",
    "\n",
    "csv_file = 'all_stt.csv'\n",
    "txt_file = 'all_stt.txt'\n",
    "with open(txt_file, \"w\", encoding = 'utf-8') as my_output_file:\n",
    "    with open(csv_file, \"r\", encoding = 'utf-8') as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tell_us', 1051),\n",
       " ('actually_tell', 686),\n",
       " ('actually_without', 201),\n",
       " ('actually_us', 199),\n",
       " ('us_without', 158)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –í—ã—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç–Ω—ã–µ –ø–∞—Ä—ã —Å–ª–æ–≤\n",
    "\n",
    "filename = 'all_stt.csv'\n",
    "\n",
    "f = open (filename,'r',encoding='utf-8')\n",
    "text = str (f.read())\n",
    "lemmatize_nltk ('TRY_lemma_list_stt.txt', text)\n",
    "with open ('TRY_lemma_list_stt.txt', 'r', encoding='utf-8') as f:\n",
    "    lemmatext = str (f.read()).split ()\n",
    "\n",
    "stop_words = get_stop_words ('english')\n",
    "stop_words.extend (['don', 't', 'gon', 'na', 'm', 's', 'part', 'Part', 'I', 'im', 'You', 'you', 'YOU', 'u', 'i', 'It', 'it',\n",
    "                    'cc', 'CC', 'Cc', 'v', 'stitch', \"n't\", 'go', 'can', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "                    'if', 'If', 'r', 'and', 'And', 'like', 'me', 'Me', 'of', 'Of', 'oh', 'hey', 'a', 'how', 'no', 'are',\n",
    "                   'in', 'the', 'did', 'do', 'didn', 'ya', 're', 'he', 'or', 'to', 'be', 'is', 'as', 'that', 'tho', 'though',\n",
    "                   'also', 'la', 've', 'em', 'ta', 'e', 'el', 'ah', 'didnt', 'dont', 'hi', 'ooh', 'mm', 'huh', 'yeah'])\n",
    "\n",
    "\n",
    "clean = []\n",
    "\n",
    "for lemma in lemmatext:\n",
    "    if lemma not in stop_words and not re.match('\\W+', lemma):\n",
    "        clean.append(lemma)\n",
    "        \n",
    "lemmatext = clean\n",
    "\n",
    "pairs = []\n",
    "\n",
    "for i in range (len(lemmatext)-1):\n",
    "    if lemmatext[i] != lemmatext[i+1]:\n",
    "        pair = min (lemmatext[i], lemmatext[i+1]) + '_' + max (lemmatext[i], lemmatext[i+1])\n",
    "        pairs.append(pair)\n",
    "\n",
    "counter_set_pairs = Counter(pairs).most_common()\n",
    "counter_set_pairs [:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['either', 'right', 'call', 'curious', 'dumbest']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ö–æ–∂–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "\n",
    "lemmatext = ['american' if x == 'america' else x for x in lemmatext]\n",
    "lemmatext = ['american' if x == 'americans' else x for x in lemmatext]\n",
    "lemmatext = ['russian' if x == 'russia' else x for x in lemmatext]\n",
    "lemmatext = ['russian' if x == 'russians' else x for x in lemmatext]\n",
    "lemmatext = ['european' if x == 'europeans' else x for x in lemmatext]\n",
    "lemmatext = ['european' if x == 'europe' else x for x in lemmatext]\n",
    "lemmatext = ['ukrainian' if x == 'ukraine' else x for x in lemmatext]\n",
    "lemmatext [:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eastern_european', 174),\n",
       " ('state_united', 70),\n",
       " ('american_just', 59),\n",
       " ('american_people', 28),\n",
       " ('american_versus', 27)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –ø–∞—Ä, –æ—Å—Ç–∞–≤–ª—è—è –ª–∏—à—å –ø–∞—Ä—ã —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º —ç—Ç–Ω–∏—á–Ω–æ—Å—Ç–µ–π –∏/ –∏–ª–∏ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–µ–π\n",
    "\n",
    "ethnolist = ['russian', 'russia', 'usa', 'american', 'america','state', 'united', 'states',\n",
    "             'slav', 'slavs', 'slavic', 'baltic', 'eastern', 'european', 'lithuania', 'lithuanian', 'polish', 'poland', \n",
    "             'ukranian', 'eastern', 'balkan', 'postsoviet', 'yugoslav', 'yugoslavia', 'yugoslavian', 'yugo',\n",
    "             'estonian', 'estonia', 'romanian', 'romania', 'latvian', 'latvia', 'ukraine', 'ukrainian',  \n",
    "             'czechia', 'czech', 'slovenia', 'slovenian', 'bosnian', 'bosnia', 'herzegovina', 'albania', 'albanian', \n",
    "            'montenegro', 'montenegrin', 'kosovo', 'kosovar', 'kosovan', 'serbia', 'serbian', 'serb', \n",
    "            'bulgaria', 'bulgarian', 'moldova', 'moldovian', 'belarus', 'belarusian', 'georgia', 'georgian', \n",
    "             'kazakh', 'kazakhstan', 'armenia', 'armenian', 'soviet', 'croatia', 'macedonia', 'macedonian', \n",
    "             'mexico', 'mexican', 'canada', 'canadian', 'australia', 'australian', 'native', 'britain', 'kingdom']\n",
    "pairs = []\n",
    "\n",
    "\n",
    "for i in range (len(lemmatext)-1):\n",
    "    if (lemmatext[i] in ethnolist) & (lemmatext[i] != lemmatext[i+1]):\n",
    "        pair = min (lemmatext[i], lemmatext[i+1]) + '_' + max (lemmatext[i], lemmatext[i+1])\n",
    "        pairs.append(pair)\n",
    "\n",
    "counter_set_pairs = Counter(pairs).most_common()\n",
    "counter_set_pairs [:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ csv-—Ñ–∞–π–ª\n",
    "\n",
    "csv = 'word1,word2,weight' + '\\n'\n",
    "\n",
    "for count in counter_set_pairs:\n",
    "    if count[1] > 2:\n",
    "        word1 = count[0].split('_')[0]\n",
    "        word2 = count[0].split('_')[1]\n",
    "        weight = str(count[1])\n",
    "        csv += word1 + ',' + word2 + ',' + weight + '\\n'\n",
    "\n",
    "with open('Stt_ethno_ETHNICITIES_counter_pairs.csv', 'w') as f:\n",
    "    f.write(csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ —Ç—ç–≥–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['americans_tags.csv',\n",
       " 'balkan_tags.csv',\n",
       " 'baltic_tags.csv',\n",
       " 'chamoney1_tags.csv',\n",
       " 'easterneuropean_tags.csv',\n",
       " 'easterneuropeancheck_tags.csv',\n",
       " 'helvetica12_tags.csv',\n",
       " 'katteryyna_tags.csv',\n",
       " 'katteryyna_tags.csv',\n",
       " 'miadio_tags.csv',\n",
       " 'monica_zielinski_tags.csv',\n",
       " 'nikiproshin_tags.csv',\n",
       " 'okaykali_tags.csv',\n",
       " 'postsoviet_tags.csv',\n",
       " 'SepVideos_tags.csv',\n",
       " 'slavic_tags.csv',\n",
       " 'straightouttarussia_tags.csv',\n",
       " 'teameffujoe_tags.csv',\n",
       " 'TellMeYouNotAmerican_tags.csv',\n",
       " 'therussianmatreshka_tags.csv',\n",
       " 'torryhermann_tags.csv',\n",
       " 'ugneexo_tags.csv',\n",
       " 'voidable_tags.csv',\n",
       " 'webkinpoodel_tags.csv']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã —Å —Ç—ç–≥–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤ –æ–¥–Ω—É —Ç–∞–±–ª–∏—Ü—É\n",
    "\n",
    "def get_folders_name_tags (filename):\n",
    "    x = filename.split ('_tags.csv')\n",
    "    return x[0]\n",
    "\n",
    "path = os.getcwd()\n",
    "folders_subfiles = []\n",
    "for i in folders_list_dir:\n",
    "    folders_subfiles.append (os.listdir (os.path.join (path, i)))\n",
    "all_csv = []\n",
    "for i in folders_subfiles:\n",
    "    for j in i:\n",
    "        if j[-8:] == 'tags.csv':\n",
    "            all_csv.append (j)\n",
    "all_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_hashtag</th>\n",
       "      <th>relative_hashtag</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>americans</td>\n",
       "      <td>fyp</td>\n",
       "      <td>caption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>americans</td>\n",
       "      <td>foryou</td>\n",
       "      <td>caption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>americans</td>\n",
       "      <td>lmao</td>\n",
       "      <td>caption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>americans</td>\n",
       "      <td>animals</td>\n",
       "      <td>caption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>americans</td>\n",
       "      <td>russia</td>\n",
       "      <td>caption</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  original_hashtag relative_hashtag   source\n",
       "0        americans              fyp  caption\n",
       "1        americans           foryou  caption\n",
       "2        americans             lmao  caption\n",
       "3        americans          animals  caption\n",
       "4        americans           russia  caption"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω—ã–µ –≤ –µ–¥–∏–Ω—É—é —Ç–∞–±–ª–∏—Ü—É\n",
    "\n",
    "all_csv_df = pd.DataFrame ()\n",
    "for i in all_csv:\n",
    "    new_df = pd.read_csv (os.path.join (path, get_folders_name_tags (i), all_csv [all_csv.index(i)]))#.drop ('Unnamed: 0', axis = 1)\n",
    "    all_csv_df = all_csv_df.append (new_df)\n",
    "    all_csv_df.reset_index()\n",
    "all_csv_df [:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –∫–∞–∫ csv-—Ñ–∞–π–ª\n",
    "\n",
    "pd.DataFrame.to_csv (all_csv_df, 'all_tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "original_hashtag  relative_hashtag\n",
       "easterneuropean   easterneuropean     1811\n",
       "                  fyp                 1368\n",
       "slavic            slavic               898\n",
       "americans         americans            819\n",
       "fyp               foryou               738\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ—Ç–Ω—ã–µ –ø–∞—Ä—ã —Ç—ç–≥–æ–≤\n",
    "\n",
    "all_csv_df.groupby(['original_hashtag','relative_hashtag']).size().sort_values(ascending=False) [:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_hashtag</th>\n",
       "      <th>relative_hashtag</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00s</td>\n",
       "      <td>00svibes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>00s</td>\n",
       "      <td>90s</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>00s</td>\n",
       "      <td>postsoviet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>00s</td>\n",
       "      <td>vibes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>00s</td>\n",
       "      <td>–∞–≥–∞—Ç–∞–∫—Ä–∏—Å—Ç–∏</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  original_hashtag relative_hashtag  count\n",
       "0              00s         00svibes      1\n",
       "1              00s              90s      1\n",
       "2              00s       postsoviet      1\n",
       "3              00s            vibes      1\n",
       "4              00s      –∞–≥–∞—Ç–∞–∫—Ä–∏—Å—Ç–∏      1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ –¥–æ–±–∞–≤–ª—è–µ–º —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å –ø–∞—Ä —Ç—ç–≥–æ–≤, —É–¥–∞–ª—è–µ–º —Ç–µ –ø–∞—Ä—ã, –≥–¥–µ –æ–±–∞ –∑–Ω–∞—á–µ–Ω–∏—è —Å–æ–≤–ø–∞–¥–∞—é—Ç \n",
    "\n",
    "tags_for_cytoscape = pd.DataFrame({'count' : all_csv_df.groupby(['original_hashtag','relative_hashtag']).size()}).reset_index()\n",
    "tags_for_cytoscape ['original_hashtag'] = tags_for_cytoscape ['original_hashtag'].str.lower()\n",
    "tags_for_cytoscape ['relative_hashtag'] = tags_for_cytoscape ['relative_hashtag'].str.lower()\n",
    "for i in tags_for_cytoscape.iterrows ():\n",
    "    if i[1]['original_hashtag'] == i[1]['relative_hashtag']:\n",
    "        tags_for_cytoscape.drop (i[0], inplace = True)\n",
    "tags_for_cytoscape [:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_hashtag</th>\n",
       "      <th>relative_hashtag</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>501</td>\n",
       "      <td>4u</td>\n",
       "      <td>funny</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>558</td>\n",
       "      <td>4u</td>\n",
       "      <td>russianvsamerican</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>582</td>\n",
       "      <td>4u</td>\n",
       "      <td>viral</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1498</td>\n",
       "      <td>americans</td>\n",
       "      <td>fyp</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2657</td>\n",
       "      <td>democrats</td>\n",
       "      <td>americans</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     original_hashtag   relative_hashtag  count\n",
       "501                4u              funny     20\n",
       "558                4u  russianvsamerican     16\n",
       "582                4u              viral     31\n",
       "1498        americans                fyp     19\n",
       "2657        democrats          americans     16"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –ø–æ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ csv-—Ñ–∞–π–ª\n",
    "\n",
    "tags_morethan_15 = tags_for_cytoscape [ tags_for_cytoscape['count'] > 15 ]\n",
    "pd.DataFrame.to_csv (tags_morethan_15, 'tags_morethan_15.csv')\n",
    "tags_morethan_15 [:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9456"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –°–º–æ—Ç—Ä–∏–º –Ω–∞ —á–∏—Å–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç—ç–≥–æ–≤\n",
    "\n",
    "len(all_csv_df['relative_hashtag'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç—ç–≥–∏ –∫–∞–∫ gexf-—Ñ–∞–π–ª\n",
    "\n",
    "import networkx as nx\n",
    "tags_to_gexf = all_csv_df.drop ('source', axis = 1)\n",
    "tags_to_gexf.columns = ['source', 'target']\n",
    "gexf_graph = nx.from_pandas_edgelist (tags_to_gexf, create_using = nx.MultiGraph ())\n",
    "nx.write_gexf (gexf_graph, 'all_tags_graph.gexf')\n",
    "print ('Gexf-—Ñ–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω—ë–Ω')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chamoney1_comments_table.csv',\n",
       " 'helvetica12_comments_table.csv',\n",
       " 'katteryyna_comments_table.csv',\n",
       " 'monica_zielinski_comments_table.csv',\n",
       " 'okaykali_comments_table.csv',\n",
       " 'SepVideos_comments_table.csv',\n",
       " 'teameffujoe_comments_table.csv',\n",
       " 'TellMeYouNotAmerican_comments_table.csv',\n",
       " 'ugneexo_comments_table.csv',\n",
       " 'voidable_comments_table.csv',\n",
       " 'voidable2_comments_table.csv',\n",
       " 'webkinpoodel_comments_table.csv']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤ –æ–¥–Ω—É —Ç–∞–±–ª–∏—Ü—É\n",
    "\n",
    "def get_folders_name_comments (filename):\n",
    "    x = filename.split ('_comments_table.csv')\n",
    "    return x[0]\n",
    "\n",
    "path = os.getcwd()\n",
    "folders_subfiles = []\n",
    "for i in folders_list_dir:\n",
    "    folders_subfiles.append (os.listdir (os.path.join (path, i)))\n",
    "all_csv = []\n",
    "for i in folders_subfiles:\n",
    "    for j in i:\n",
    "        if j[-18:] == 'comments_table.csv':\n",
    "            all_csv.append (j)\n",
    "all_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aweme_id</th>\n",
       "      <th>cid</th>\n",
       "      <th>comment_type</th>\n",
       "      <th>create_time</th>\n",
       "      <th>date</th>\n",
       "      <th>digg_count</th>\n",
       "      <th>is_author_digged</th>\n",
       "      <th>label_list</th>\n",
       "      <th>label_text</th>\n",
       "      <th>label_type</th>\n",
       "      <th>...</th>\n",
       "      <th>reply_to_reply_id</th>\n",
       "      <th>status</th>\n",
       "      <th>stick_position</th>\n",
       "      <th>text</th>\n",
       "      <th>text_extra</th>\n",
       "      <th>user</th>\n",
       "      <th>user_buried</th>\n",
       "      <th>user_nickname</th>\n",
       "      <th>user_sec_uid</th>\n",
       "      <th>user_unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6935766580089359621</td>\n",
       "      <td>6935892084100907009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1614888230</td>\n",
       "      <td>04.03.2021</td>\n",
       "      <td>3263</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>–≤–æ–ø—Ä–æ—Å —Ç–æ –≤ –¥—Ä—É–≥–æ–º —è–±–ª–æ—á–Ω—ã–π —Å–ø–∞—Å –∫–æ–≥–¥–∞?</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'nickname': '—Ñ–∞–ª–∞—Ñ–µ–ª—å', 'unique_id': 'felevel...</td>\n",
       "      <td>False</td>\n",
       "      <td>—Ñ–∞–ª–∞—Ñ–µ–ª—å</td>\n",
       "      <td>MS4wLjABAAAAqbCfA702eLQZWdfmozv2kT1hOcuIjaVUYx...</td>\n",
       "      <td>felevel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6935766580089359621</td>\n",
       "      <td>6935899153248124929</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1614889877</td>\n",
       "      <td>04.03.2021</td>\n",
       "      <td>1543</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>–ö–û–ì–î–ê –ë–´–õ–û –ö–†–ï–©–ï–ù–ò–ï –†–£–°–ò, –ê????</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'nickname': '–ë–µ—Ä–µ–ª–∏', 'unique_id': 'mybeleri'...</td>\n",
       "      <td>False</td>\n",
       "      <td>–ë–µ—Ä–µ–ª–∏</td>\n",
       "      <td>MS4wLjABAAAA-6yrXI7_nkgLRzwNkeRINqtskKxwEvnmtj...</td>\n",
       "      <td>mybeleri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6935766580089359621</td>\n",
       "      <td>6935873418386784257</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1614883876</td>\n",
       "      <td>04.03.2021</td>\n",
       "      <td>2068</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>–¥–∞—Ç–∞ –ª–µ–¥–æ–≤–æ–≥–æ –ø–æ–±–æ–∏—â–∞...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'nickname': 'ArinessA', 'unique_id': 'aderin_...</td>\n",
       "      <td>False</td>\n",
       "      <td>ArinessA</td>\n",
       "      <td>MS4wLjABAAAAT88x2sKkWDRlZxizL315wyvIxlaK1o3CqV...</td>\n",
       "      <td>aderin_arr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6935766580089359621</td>\n",
       "      <td>6935997455586869250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1614912765</td>\n",
       "      <td>05.03.2021</td>\n",
       "      <td>1089</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ —á—ë —Å –Ω–∏–º –±—É–¥–µ—Ç, –∫–æ–≥–¥–∞ —É–∑–Ω–∞–µ—Ç, —á—Ç–æ –º—ã...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'nickname': '—Ö–∏–ø—Ö–æ–ø–æ—Ç–∞–º', 'unique_id': 'hiph0...</td>\n",
       "      <td>False</td>\n",
       "      <td>—Ö–∏–ø—Ö–æ–ø–æ—Ç–∞–º</td>\n",
       "      <td>MS4wLjABAAAAdU9OtwQGEREb6DNtyNhMwWEBMIraacHxsX...</td>\n",
       "      <td>hiph0p0p0tamus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6935766580089359621</td>\n",
       "      <td>6936754794384752641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1615089088</td>\n",
       "      <td>07.03.2021</td>\n",
       "      <td>1001</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>–ü–æ—á–µ–º—É —Ç–æ –∞–º–µ—Ä–∏–∫–∞–Ω—Ü—ã —Å—á–∏—Ç–∞—é—Ç, —á—Ç–æ –≤—Å–µ –ª—é–¥–∏ –≤ –º...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'nickname': 'diankane', 'unique_id': 'diankan...</td>\n",
       "      <td>False</td>\n",
       "      <td>diankane</td>\n",
       "      <td>MS4wLjABAAAAbgzqiTODtetF9BBXazySg3ec3GvF4VIPol...</td>\n",
       "      <td>diankane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              aweme_id                  cid  comment_type  create_time  \\\n",
       "0  6935766580089359621  6935892084100907009           NaN   1614888230   \n",
       "1  6935766580089359621  6935899153248124929           NaN   1614889877   \n",
       "2  6935766580089359621  6935873418386784257           NaN   1614883876   \n",
       "3  6935766580089359621  6935997455586869250           NaN   1614912765   \n",
       "4  6935766580089359621  6936754794384752641           NaN   1615089088   \n",
       "\n",
       "         date  digg_count  is_author_digged label_list label_text  label_type  \\\n",
       "0  04.03.2021        3263             False        NaN        NaN         NaN   \n",
       "1  04.03.2021        1543             False        NaN        NaN         NaN   \n",
       "2  04.03.2021        2068             False        NaN        NaN         NaN   \n",
       "3  05.03.2021        1089             False        NaN        NaN         NaN   \n",
       "4  07.03.2021        1001             False        NaN        NaN         NaN   \n",
       "\n",
       "   ... reply_to_reply_id  status  stick_position  \\\n",
       "0  ...               0.0     1.0             0.0   \n",
       "1  ...               0.0     1.0             0.0   \n",
       "2  ...               0.0     1.0             0.0   \n",
       "3  ...               0.0     1.0             0.0   \n",
       "4  ...               0.0     1.0             0.0   \n",
       "\n",
       "                                                text  text_extra  \\\n",
       "0            –≤–æ–ø—Ä–æ—Å —Ç–æ –≤ –¥—Ä—É–≥–æ–º —è–±–ª–æ—á–Ω—ã–π —Å–ø–∞—Å –∫–æ–≥–¥–∞?          []   \n",
       "1                    –ö–û–ì–î–ê –ë–´–õ–û –ö–†–ï–©–ï–ù–ò–ï –†–£–°–ò, –ê????          []   \n",
       "2                           –¥–∞—Ç–∞ –ª–µ–¥–æ–≤–æ–≥–æ –ø–æ–±–æ–∏—â–∞...          []   \n",
       "3  –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ —á—ë —Å –Ω–∏–º –±—É–¥–µ—Ç, –∫–æ–≥–¥–∞ —É–∑–Ω–∞–µ—Ç, —á—Ç–æ –º—ã...          []   \n",
       "4  –ü–æ—á–µ–º—É —Ç–æ –∞–º–µ—Ä–∏–∫–∞–Ω—Ü—ã —Å—á–∏—Ç–∞—é—Ç, —á—Ç–æ –≤—Å–µ –ª—é–¥–∏ –≤ –º...          []   \n",
       "\n",
       "                                                user user_buried  \\\n",
       "0  {'nickname': '—Ñ–∞–ª–∞—Ñ–µ–ª—å', 'unique_id': 'felevel...       False   \n",
       "1  {'nickname': '–ë–µ—Ä–µ–ª–∏', 'unique_id': 'mybeleri'...       False   \n",
       "2  {'nickname': 'ArinessA', 'unique_id': 'aderin_...       False   \n",
       "3  {'nickname': '—Ö–∏–ø—Ö–æ–ø–æ—Ç–∞–º', 'unique_id': 'hiph0...       False   \n",
       "4  {'nickname': 'diankane', 'unique_id': 'diankan...       False   \n",
       "\n",
       "  user_nickname                                       user_sec_uid  \\\n",
       "0      —Ñ–∞–ª–∞—Ñ–µ–ª—å  MS4wLjABAAAAqbCfA702eLQZWdfmozv2kT1hOcuIjaVUYx...   \n",
       "1        –ë–µ—Ä–µ–ª–∏  MS4wLjABAAAA-6yrXI7_nkgLRzwNkeRINqtskKxwEvnmtj...   \n",
       "2      ArinessA  MS4wLjABAAAAT88x2sKkWDRlZxizL315wyvIxlaK1o3CqV...   \n",
       "3    —Ö–∏–ø—Ö–æ–ø–æ—Ç–∞–º  MS4wLjABAAAAdU9OtwQGEREb6DNtyNhMwWEBMIraacHxsX...   \n",
       "4      diankane  MS4wLjABAAAAbgzqiTODtetF9BBXazySg3ec3GvF4VIPol...   \n",
       "\n",
       "   user_unique_id  \n",
       "0         felevel  \n",
       "1        mybeleri  \n",
       "2      aderin_arr  \n",
       "3  hiph0p0p0tamus  \n",
       "4        diankane  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã –≤ –µ–¥–∏–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º\n",
    "\n",
    "all_csv_df = pd.DataFrame ()\n",
    "for i in all_csv:\n",
    "    try:\n",
    "        new_df = pd.read_csv (os.path.join (path, get_folders_name_comments (i), all_csv [all_csv.index(i)]), sep=';').drop ('Unnamed: 0', axis = 1)\n",
    "        all_csv_df = all_csv_df.append (new_df)\n",
    "        all_csv_df.reset_index()\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "all_csv_df = all_csv_df.drop_duplicates()\n",
    "all_csv_df [:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–≤–æ–ø—Ä–æ—Å —Ç–æ –≤ –¥—Ä—É–≥–æ–º —è–±–ª–æ—á–Ω—ã–π —Å–ø–∞—Å –∫–æ–≥–¥–∞?',\n",
       " '–∫–æ–≥–¥–∞ –±—ã–ª–æ –∫—Ä–µ—â–µ–Ω–∏–µ —Ä—É—Å–∏, –∞????',\n",
       " '–¥–∞—Ç–∞ –ª–µ–¥–æ–≤–æ–≥–æ –ø–æ–±–æ–∏—â–∞...',\n",
       " '–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ —á—ë —Å –Ω–∏–º –±—É–¥–µ—Ç, –∫–æ–≥–¥–∞ —É–∑–Ω–∞–µ—Ç, —á—Ç–æ –º—ã –≤ –∫—É—Ä—Å–µ –∏ –∫–æ–≥–¥–∞ —É –Ω–∏—Ö –¥–µ–Ω—å –±–ª–∞–≥–æ–¥–∞—Ä–µ–Ω–∏—è',\n",
       " '–ø–æ—á–µ–º—É —Ç–æ –∞–º–µ—Ä–∏–∫–∞–Ω—Ü—ã —Å—á–∏—Ç–∞—é—Ç, —á—Ç–æ –≤—Å–µ –ª—é–¥–∏ –≤ –º–∏—Ä–µ –¥–æ–ª–∂–Ω—ã –∑–Ω–∞—Ç—å –∏—Ö –∫—É–ª—å—Ç—É—Ä—É –∏ –æ–±—ã—á–∞–∏, –∫–æ–≥–¥–∞ –æ–Ω–∏ —Å–∞–º–∏ –Ω–µ –∑–Ω–∞—é—Ç –æ –≤–Ω–µ—à–Ω–µ–º –º–∏—Ä–µ –Ω–∏—á–µ–≥–æ... –±—É–∫–≤–∞–ª—å–Ω–æ.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –≤ –µ–¥–∏–Ω—ã–π —Å–ø–∏—Å–æ–∫\n",
    "\n",
    "all_comments = []\n",
    "for i in all_csv_df ['text']:\n",
    "    try:\n",
    "        all_comments.append (i.lower())\n",
    "    except TypeError:\n",
    "        pass\n",
    "all_comments [:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫–∞–∫ csv-—Ñ–∞–π–ª\n",
    "\n",
    "with open ('all_comments.csv', 'w', newline = '', encoding = 'utf-8') as all_comments_file:\n",
    "    writer = csv.writer (all_comments_file, delimiter = \",\")\n",
    "    for i in all_comments:\n",
    "        print (i)\n",
    "        writer.writerow ([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∫–∞–∫ txt-—Ñ–∞–π–ª\n",
    "\n",
    "csv_file = 'all_comments.csv'\n",
    "txt_file = 'all_comments.txt'\n",
    "with open(txt_file, \"w\", encoding = 'utf-8') as my_output_file:\n",
    "    with open(csv_file, \"r\", encoding = 'utf-8') as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('american_ask', 177),\n",
       " ('america_south', 163),\n",
       " ('american_say', 137),\n",
       " ('american_think', 127),\n",
       " ('english_speak', 101)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª –∏ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º –µ–≥–æ, –æ—á–∏—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –ª–µ–º–º –æ—Ç —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "\n",
    "filename = 'all_comments.csv'\n",
    "\n",
    "f = open (filename,'r',encoding='utf-8')\n",
    "text = str (f.read())\n",
    "\n",
    "lemmatize_nltk ('TRY_lemma_list.txt', text)\n",
    "\n",
    "with open ('TRY_lemma_list.txt', 'r', encoding='utf-8') as f:\n",
    "    lemmatext = str (f.read()).split ()\n",
    "    \n",
    "stop_words = get_stop_words ('english')\n",
    "stop_words.extend (['don', 't', 'gon', 'na', 'm', 's', 'part', 'Part', 'I', 'im', 'You', 'you', 'YOU', 'u', 'i', 'It', 'it',\n",
    "                    'cc', 'CC', 'Cc', 'v', 'stitch', \"n't\", 'go', 'can', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "                    'if', 'If', 'r', 'and', 'And', 'like', 'me', 'Me', 'of', 'Of', 'oh', 'hey', 'a', 'how', 'no', 'are',\n",
    "                   'in', 'the', 'did', 'do', 'didn', 'ya', 're', 'he', 'or', 'to', 'be', 'is', 'as', 'that', 'tho', 'though',\n",
    "                   'also', 'la', 've', 'em', 'ta', 'e', 'el', 'ah', 'didnt', 'dont'])\n",
    "\n",
    "clean = []\n",
    "\n",
    "for lemma in lemmatext:\n",
    "    if lemma not in stop_words and not re.match('\\W+', lemma):\n",
    "        clean.append(lemma)\n",
    "        \n",
    "lemmatext = clean\n",
    "\n",
    "pairs = []\n",
    "\n",
    "for i in range (len(lemmatext)-1):\n",
    "    if lemmatext[i] != lemmatext[i+1]:\n",
    "        pair = min (lemmatext[i], lemmatext[i+1]) + '_' + max (lemmatext[i], lemmatext[i+1])\n",
    "        pairs.append(pair.lower())\n",
    "        \n",
    "counter_set_pairs = Counter(pairs).most_common()\n",
    "counter_set_pairs [:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–∏—Å–∫ –ø–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º\n",
    "\n",
    "for i in all_comments:\n",
    "    if ('health' in i) & ('care' in i):\n",
    "        print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–∫ csv-—Ñ–∞–π–ª –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ –≥—Ä–∞—Ñ\n",
    "\n",
    "csv = 'word1,word2,weight' + '\\n'\n",
    "\n",
    "for count in counter_set_pairs:\n",
    "    if count[1] > 10:\n",
    "        word1 = count[0].split('_')[0]\n",
    "        word2 = count[0].split('_')[1]\n",
    "        weight = str(count[1])\n",
    "        csv += word1 + ',' + word2 + ',' + weight + '\\n'\n",
    "        \n",
    "with open('CommentsPairs.csv', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(csv)\n",
    "    \n",
    "pd.read_csv ('CommentsPairs.csv', encoding = 'utf-8') [:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('american_ask', 152),\n",
       " ('american_say', 99),\n",
       " ('state_united', 95),\n",
       " ('american_think', 93),\n",
       " ('american_know', 60)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –í—ã–¥–µ–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–∞—Ä—ã, –≥–¥–µ —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è —ç—Ç–Ω–∏—á–Ω–æ—Å—Ç—å –∏–ª–∏ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–∞—è –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å\n",
    "\n",
    "ethnolist = ['russian', 'russia', 'usa', 'american', 'america', 'america', 'state', 'united', \n",
    "             'slav', 'slavic', 'baltic', 'easterneuropean', 'lithuania', 'lithuanian', 'polish', 'poland', \n",
    "             'ukranian', 'eastern', 'balkan', 'baltic', 'postsoviet', 'yugoslav', 'yugoslavia', 'yugoslavian',\n",
    "             'estonian', 'estonia', 'romanian', 'romania', 'latvian', 'latvia', 'ukraine', 'ukranian',  \n",
    "             'czechia', 'czech', 'slovenia', 'slovenian', 'bosnian', 'bosnia', 'bosniaandherzegovina', 'albania', 'albanian', \n",
    "            'montenegro', 'montenegrin', 'kosovo', 'kosovar', 'kosovan', 'serbia', 'serbian', 'serb', \n",
    "            'bulgaria', 'bulgarian', 'moldova', 'moldovian', 'belarus', 'belarusian', 'georgia', 'georgian', \n",
    "             'kazakh', 'kazakhstan', 'armenia', 'armenian']\n",
    "pairs = []\n",
    "\n",
    "for i in range (len(lemmatext)-1):\n",
    "    if (lemmatext[i] in ethnolist) & (lemmatext[i] != lemmatext[i+1]):\n",
    "        pair = min (lemmatext[i], lemmatext[i+1]) + '_' + max (lemmatext[i], lemmatext[i+1])\n",
    "        pairs.append(pair.lower())\n",
    "\n",
    "counter_set_pairs = Counter(pairs).most_common()\n",
    "counter_set_pairs [:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ê–≤—Ç–æ—Ä –∫–æ–¥–∞: –ú–∞—Ä–∏—è –ö–∞–∑–∞–∫–æ–≤–∞ (@undine_su_menulio), marikasakowa@gmail.com \n",
    "\n",
    "–ú–æ—Å–∫–≤–∞, 2021 –≥–æ–¥."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
